{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-02-16T07:23:13.709724Z","iopub.status.busy":"2024-02-16T07:23:13.708961Z","iopub.status.idle":"2024-02-16T07:23:14.851564Z","shell.execute_reply":"2024-02-16T07:23:14.850372Z","shell.execute_reply.started":"2024-02-16T07:23:13.709689Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["/kaggle/input/imdb-dataset-of-50k-movie-reviews/IMDB Dataset.csv\n"]}],"source":["# This Python 3 environment comes with many helpful analytics libraries installed\n","# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n","# For example, here's several helpful packages to load\n","\n","import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","\n","# Input data files are available in the read-only \"../input/\" directory\n","# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n","\n","import os\n","for dirname, _, filenames in os.walk('/kaggle/input'):\n","    for filename in filenames:\n","        print(os.path.join(dirname, filename))\n","\n","# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n","# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-02-17T06:20:45.343424Z","iopub.status.busy":"2024-02-17T06:20:45.342971Z","iopub.status.idle":"2024-02-17T06:20:47.072637Z","shell.execute_reply":"2024-02-17T06:20:47.071193Z","shell.execute_reply.started":"2024-02-17T06:20:45.343394Z"},"trusted":true},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","df = pd.read_csv(r'/kaggle/input/imdb-dataset-of-50k-movie-reviews/IMDB Dataset.csv')"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-02-17T06:20:48.242360Z","iopub.status.busy":"2024-02-17T06:20:48.242029Z","iopub.status.idle":"2024-02-17T06:20:48.270767Z","shell.execute_reply":"2024-02-17T06:20:48.269065Z","shell.execute_reply.started":"2024-02-17T06:20:48.242331Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>review</th>\n","      <th>sentiment</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>20535</th>\n","      <td>Well, this film came on on a workday at 230am....</td>\n","      <td>positive</td>\n","    </tr>\n","    <tr>\n","      <th>47739</th>\n","      <td>This movie is essentially a \"how-to\" on how to...</td>\n","      <td>negative</td>\n","    </tr>\n","    <tr>\n","      <th>24505</th>\n","      <td>The movie was completely misleading and the bo...</td>\n","      <td>negative</td>\n","    </tr>\n","    <tr>\n","      <th>17719</th>\n","      <td>The Sea is Watching was an interesting film ex...</td>\n","      <td>positive</td>\n","    </tr>\n","    <tr>\n","      <th>37034</th>\n","      <td>To answer the question of a previous reviewer ...</td>\n","      <td>positive</td>\n","    </tr>\n","    <tr>\n","      <th>24499</th>\n","      <td>It is not un-common to see U.S. re-makes of fo...</td>\n","      <td>negative</td>\n","    </tr>\n","    <tr>\n","      <th>15072</th>\n","      <td>Set in 1962 Hong Kong (in turbulent times, as ...</td>\n","      <td>positive</td>\n","    </tr>\n","    <tr>\n","      <th>15120</th>\n","      <td>La Maman et la Putain has to be watched as a m...</td>\n","      <td>positive</td>\n","    </tr>\n","    <tr>\n","      <th>46710</th>\n","      <td>I enjoyed this film. I thought it was an excel...</td>\n","      <td>positive</td>\n","    </tr>\n","    <tr>\n","      <th>33533</th>\n","      <td>Brand Hauser (John Cusack) is an assassin for ...</td>\n","      <td>positive</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                                  review sentiment\n","20535  Well, this film came on on a workday at 230am....  positive\n","47739  This movie is essentially a \"how-to\" on how to...  negative\n","24505  The movie was completely misleading and the bo...  negative\n","17719  The Sea is Watching was an interesting film ex...  positive\n","37034  To answer the question of a previous reviewer ...  positive\n","24499  It is not un-common to see U.S. re-makes of fo...  negative\n","15072  Set in 1962 Hong Kong (in turbulent times, as ...  positive\n","15120  La Maman et la Putain has to be watched as a m...  positive\n","46710  I enjoyed this film. I thought it was an excel...  positive\n","33533  Brand Hauser (John Cusack) is an assassin for ...  positive"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["df.sample(10)"]},{"cell_type":"markdown","metadata":{},"source":["# Data preproccessing "]},{"cell_type":"markdown","metadata":{},"source":["## Step - 1\n"," - lower casing all string\n"," "]},{"cell_type":"markdown","metadata":{},"source":[]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-02-17T06:20:55.751723Z","iopub.status.busy":"2024-02-17T06:20:55.751382Z","iopub.status.idle":"2024-02-17T06:20:55.930966Z","shell.execute_reply":"2024-02-17T06:20:55.929143Z","shell.execute_reply.started":"2024-02-17T06:20:55.751695Z"},"trusted":true},"outputs":[],"source":["df['review'] = df['review'].str.lower()\n"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-02-17T06:20:55.988293Z","iopub.status.busy":"2024-02-17T06:20:55.987902Z","iopub.status.idle":"2024-02-17T06:20:55.994696Z","shell.execute_reply":"2024-02-17T06:20:55.993796Z","shell.execute_reply.started":"2024-02-17T06:20:55.988263Z"},"trusted":true},"outputs":[{"data":{"text/plain":["\"basically there's a family where a little boy (jake) thinks there's a zombie in his closet & his parents are fighting all the time.<br /><br />this movie is slower than a soap opera... and suddenly, jake decides to become rambo and kill the zombie.<br /><br />ok, first of all when you're going to make a film you must decide if its a thriller or a drama! as a drama the movie is watchable. parents are divorcing & arguing like in real life. and then we have jake with his closet which totally ruins all the film! i expected to see a boogeyman similar movie, and instead i watched a drama with some meaningless thriller spots.<br /><br />3 out of 10 just for the well playing parents & descent dialogs. as for the shots with jake: just ignore them.\""]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["df['review'][3]"]},{"cell_type":"markdown","metadata":{},"source":["## Step - 2\n","- Removing HTML tags"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-17T06:20:59.211965Z","iopub.status.busy":"2024-02-17T06:20:59.211580Z","iopub.status.idle":"2024-02-17T06:20:59.217388Z","shell.execute_reply":"2024-02-17T06:20:59.216105Z","shell.execute_reply.started":"2024-02-17T06:20:59.211937Z"},"trusted":true},"outputs":[],"source":["# Using python regularexpressions\n","import re\n","def remove_html_tags(text):\n","    pattern = re.compile('<.*?>')\n","    return pattern.sub(r'',text) # substituting html tags to space"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-02-17T06:20:59.588647Z","iopub.status.busy":"2024-02-17T06:20:59.588284Z","iopub.status.idle":"2024-02-17T06:20:59.737877Z","shell.execute_reply":"2024-02-17T06:20:59.736892Z","shell.execute_reply.started":"2024-02-17T06:20:59.588620Z"},"trusted":true},"outputs":[],"source":["df['review']= df['review'].apply(remove_html_tags)"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-02-17T06:21:00.277835Z","iopub.status.busy":"2024-02-17T06:21:00.277485Z","iopub.status.idle":"2024-02-17T06:21:00.286483Z","shell.execute_reply":"2024-02-17T06:21:00.285163Z","shell.execute_reply.started":"2024-02-17T06:21:00.277811Z"},"trusted":true},"outputs":[{"data":{"text/plain":["\"basically there's a family where a little boy (jake) thinks there's a zombie in his closet & his parents are fighting all the time.this movie is slower than a soap opera... and suddenly, jake decides to become rambo and kill the zombie.ok, first of all when you're going to make a film you must decide if its a thriller or a drama! as a drama the movie is watchable. parents are divorcing & arguing like in real life. and then we have jake with his closet which totally ruins all the film! i expected to see a boogeyman similar movie, and instead i watched a drama with some meaningless thriller spots.3 out of 10 just for the well playing parents & descent dialogs. as for the shots with jake: just ignore them.\""]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["df['review'][3]"]},{"cell_type":"markdown","metadata":{},"source":["## Step - 3\n","- Removing Urls"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-02-17T06:21:05.937601Z","iopub.status.busy":"2024-02-17T06:21:05.937249Z","iopub.status.idle":"2024-02-17T06:21:05.943192Z","shell.execute_reply":"2024-02-17T06:21:05.941628Z","shell.execute_reply.started":"2024-02-17T06:21:05.937573Z"},"trusted":true},"outputs":[],"source":["import re\n","def remove_url(text):\n","    pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n","    return pattern.sub(r'',text)"]},{"cell_type":"markdown","metadata":{},"source":["## Step - 4\n","- Removing Punctuation\n","- These signs are known as punchations( !\"#$%&'()*+, -./:;<=>?@[\\]^_`{|}~)\n"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2024-02-17T06:21:12.925269Z","iopub.status.busy":"2024-02-17T06:21:12.924946Z","iopub.status.idle":"2024-02-17T06:21:12.931513Z","shell.execute_reply":"2024-02-17T06:21:12.930182Z","shell.execute_reply.started":"2024-02-17T06:21:12.925243Z"},"trusted":true},"outputs":[{"data":{"text/plain":["'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["import string, time\n","string.punctuation"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2024-02-17T06:21:14.589273Z","iopub.status.busy":"2024-02-17T06:21:14.588927Z","iopub.status.idle":"2024-02-17T06:21:14.593920Z","shell.execute_reply":"2024-02-17T06:21:14.593093Z","shell.execute_reply.started":"2024-02-17T06:21:14.589246Z"},"trusted":true},"outputs":[],"source":["exclude = string.punctuation"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2024-02-17T06:21:17.227477Z","iopub.status.busy":"2024-02-17T06:21:17.227140Z","iopub.status.idle":"2024-02-17T06:21:17.233140Z","shell.execute_reply":"2024-02-17T06:21:17.231636Z","shell.execute_reply.started":"2024-02-17T06:21:17.227449Z"},"trusted":true},"outputs":[],"source":["def remove_punch(text):\n","    for char in exclude:\n","        text = text.replce(char,'')\n","    return text\n","# Very slow"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2024-02-17T06:21:19.275538Z","iopub.status.busy":"2024-02-17T06:21:19.275153Z","iopub.status.idle":"2024-02-17T06:21:19.280895Z","shell.execute_reply":"2024-02-17T06:21:19.279153Z","shell.execute_reply.started":"2024-02-17T06:21:19.275507Z"},"trusted":true},"outputs":[],"source":["def remove_punch1(text):\n","    return text.translate(str.maketrans('','',exclude))"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2024-02-17T06:21:20.050238Z","iopub.status.busy":"2024-02-17T06:21:20.049908Z","iopub.status.idle":"2024-02-17T06:21:21.251728Z","shell.execute_reply":"2024-02-17T06:21:21.250715Z","shell.execute_reply.started":"2024-02-17T06:21:20.050211Z"},"trusted":true},"outputs":[],"source":["df['review']= df['review'].apply(remove_punch1)"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2024-02-17T06:21:22.892711Z","iopub.status.busy":"2024-02-17T06:21:22.892382Z","iopub.status.idle":"2024-02-17T06:21:22.910015Z","shell.execute_reply":"2024-02-17T06:21:22.909026Z","shell.execute_reply.started":"2024-02-17T06:21:22.892685Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>review</th>\n","      <th>sentiment</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>21207</th>\n","      <td>at your video store you might find this gemthe...</td>\n","      <td>positive</td>\n","    </tr>\n","    <tr>\n","      <th>21190</th>\n","      <td>i was at school in the late sixties and early ...</td>\n","      <td>positive</td>\n","    </tr>\n","    <tr>\n","      <th>7950</th>\n","      <td>spoilersone of the worst films ive seen since...</td>\n","      <td>negative</td>\n","    </tr>\n","    <tr>\n","      <th>22812</th>\n","      <td>boogie nights was without a doubt the best fil...</td>\n","      <td>positive</td>\n","    </tr>\n","    <tr>\n","      <th>39723</th>\n","      <td>this is very nearly a perfect film the ideas w...</td>\n","      <td>positive</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                                  review sentiment\n","21207  at your video store you might find this gemthe...  positive\n","21190  i was at school in the late sixties and early ...  positive\n","7950    spoilersone of the worst films ive seen since...  negative\n","22812  boogie nights was without a doubt the best fil...  positive\n","39723  this is very nearly a perfect film the ideas w...  positive"]},"execution_count":15,"metadata":{},"output_type":"execute_result"}],"source":["   df.sample(5)"]},{"cell_type":"markdown","metadata":{},"source":["## Step - 5\n","- Chat word treatment\n"]},{"cell_type":"code","execution_count":51,"metadata":{"execution":{"iopub.execute_input":"2024-02-16T07:52:15.594573Z","iopub.status.busy":"2024-02-16T07:52:15.593858Z","iopub.status.idle":"2024-02-16T07:52:15.604971Z","shell.execute_reply":"2024-02-16T07:52:15.603831Z","shell.execute_reply.started":"2024-02-16T07:52:15.594531Z"},"trusted":true},"outputs":[],"source":["abbreviations_string = \"\"\"\n","AFAIK=As Far As I Know\n","AFK=Away From Keyboard\n","ASAP=As Soon As Possible\n","ATK=At The Keyboard\n","ATM=At The Moment\n","A3=Anytime, Anywhere, Anyplace\n","BAK=Back At Keyboard\n","BBL=Be Back Later\n","BBS=Be Back Soon\n","BFN=Bye For Now\n","B4N=Bye For Now\n","BRB=Be Right Back\n","BRT=Be Right There\n","BTW=By The Way\n","B4=Before\n","B4N=Bye For Now\n","CU=See You\n","CUL8R=See You Later\n","CYA=See You\n","FAQ=Frequently Asked Questions\n","FC=Fingers Crossed\n","FWIW=For What It's Worth\n","FYI=For Your Information\n","GAL=Get A Life\n","GG=Good Game\n","GN=Good Night\n","GMTA=Great Minds Think Alike\n","GR8=Great!\n","G9=Genius\n","IC=I See\n","ICQ=I Seek you (also a chat program)\n","ILU=ILU: I Love You\n","IMHO=In My Honest/Humble Opinion\n","IMO=In My Opinion\n","IOW=In Other Words\n","IRL=In Real Life\n","KISS=Keep It Simple, Stupid\n","LDR=Long Distance Relationship\n","LMAO=Laugh My A.. Off\n","LOL=Laughing Out Loud\n","LTNS=Long Time No See\n","L8R=Later\n","MTE=My Thoughts Exactly\n","M8=Mate\n","NRN=No Reply Necessary\n","OIC=Oh I See\n","PITA=Pain In The A..\n","PRT=Party\n","PRW=Parents Are Watching\n","QPSA?\tQue Pasa?\n","ROFL=Rolling On The Floor Laughing\n","ROFLOL=Rolling On The Floor Laughing Out Loud\n","ROTFLMAO=Rolling On The Floor Laughing My A.. Off\n","SK8=Skate\n","STATS=Your sex and age\n","ASL=Age, Sex, Location\n","THX=Thank You\n","TTFN=Ta-Ta For Now!\n","TTYL=Talk To You Later\n","U=You\n","U2=You Too\n","U4E=Yours For Ever\n","WB=Welcome Back\n","WTF=What The F...\n","WTG=Way To Go!\n","WUF=Where Are You From?\n","W8=Wait...\n","7K=Sick:-D Laugher\n","TFW=That feeling when\n","MFW=My face when\n","MRW=My reaction when\n","IFYP=I feel your pain\n","LOL=Laughing out loud\n","TNTL=Trying not to laugh\n","JK=Just kidding\n","IDC=I don’t care\n","ILY=I love you\n","IMU=I miss you\n","ADIH=Another day in hell\n","IDC=I don’t care\n","ZZZ=Sleeping, bored, tired\n","WYWH=Wish you were here\n","TIME=Tears in my eyes\n","BAE=Before anyone else\n","FIMH=Forever in my heart\n","BSAAW=Big smile and a wink\n","BWL=Bursting with laughter\n","LMAO=Laughing my a** off\n","BFF=Best friends forever\n","CSL=Can’t stop laughing\n","\"\"\"\n","\n"]},{"cell_type":"code","execution_count":52,"metadata":{"execution":{"iopub.execute_input":"2024-02-16T07:52:15.608340Z","iopub.status.busy":"2024-02-16T07:52:15.607888Z","iopub.status.idle":"2024-02-16T07:52:15.622985Z","shell.execute_reply":"2024-02-16T07:52:15.621629Z","shell.execute_reply.started":"2024-02-16T07:52:15.608301Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["{'AFAIK': 'As Far As I Know', 'AFK': 'Away From Keyboard', 'ASAP': 'As Soon As Possible', 'ATK': 'At The Keyboard', 'ATM': 'At The Moment', 'A3': 'Anytime, Anywhere, Anyplace', 'BAK': 'Back At Keyboard', 'BBL': 'Be Back Later', 'BBS': 'Be Back Soon', 'BFN': 'Bye For Now', 'B4N': 'Bye For Now', 'BRB': 'Be Right Back', 'BRT': 'Be Right There', 'BTW': 'By The Way', 'B4': 'Before', 'CU': 'See You', 'CUL8R': 'See You Later', 'CYA': 'See You', 'FAQ': 'Frequently Asked Questions', 'FC': 'Fingers Crossed', 'FWIW': \"For What It's Worth\", 'FYI': 'For Your Information', 'GAL': 'Get A Life', 'GG': 'Good Game', 'GN': 'Good Night', 'GMTA': 'Great Minds Think Alike', 'GR8': 'Great!', 'G9': 'Genius', 'IC': 'I See', 'ICQ': 'I Seek you (also a chat program)', 'ILU': 'ILU: I Love You', 'IMHO': 'In My Honest/Humble Opinion', 'IMO': 'In My Opinion', 'IOW': 'In Other Words', 'IRL': 'In Real Life', 'KISS': 'Keep It Simple, Stupid', 'LDR': 'Long Distance Relationship', 'LMAO': 'Laughing my a** off', 'LOL': 'Laughing out loud', 'LTNS': 'Long Time No See', 'L8R': 'Later', 'MTE': 'My Thoughts Exactly', 'M8': 'Mate', 'NRN': 'No Reply Necessary', 'OIC': 'Oh I See', 'PITA': 'Pain In The A..', 'PRT': 'Party', 'PRW': 'Parents Are Watching', 'ROFL': 'Rolling On The Floor Laughing', 'ROFLOL': 'Rolling On The Floor Laughing Out Loud', 'ROTFLMAO': 'Rolling On The Floor Laughing My A.. Off', 'SK8': 'Skate', 'STATS': 'Your sex and age', 'ASL': 'Age, Sex, Location', 'THX': 'Thank You', 'TTFN': 'Ta-Ta For Now!', 'TTYL': 'Talk To You Later', 'U': 'You', 'U2': 'You Too', 'U4E': 'Yours For Ever', 'WB': 'Welcome Back', 'WTF': 'What The F...', 'WTG': 'Way To Go!', 'WUF': 'Where Are You From?', 'W8': 'Wait...', '7K': 'Sick:-D Laugher', 'TFW': 'That feeling when', 'MFW': 'My face when', 'MRW': 'My reaction when', 'IFYP': 'I feel your pain', 'TNTL': 'Trying not to laugh', 'JK': 'Just kidding', 'IDC': 'I don’t care', 'ILY': 'I love you', 'IMU': 'I miss you', 'ADIH': 'Another day in hell', 'ZZZ': 'Sleeping, bored, tired', 'WYWH': 'Wish you were here', 'TIME': 'Tears in my eyes', 'BAE': 'Before anyone else', 'FIMH': 'Forever in my heart', 'BSAAW': 'Big smile and a wink', 'BWL': 'Bursting with laughter', 'BFF': 'Best friends forever', 'CSL': 'Can’t stop laughing'}\n"]}],"source":["abbreviations_dict = {}\n","lines = abbreviations_string.strip().split('\\n')\n","\n","for line in lines:\n","    if '=' in line:\n","        abbreviation, meaning = line.split('=')\n","        abbreviations_dict[abbreviation.strip()] = meaning.strip()\n","print(abbreviations_dict)"]},{"cell_type":"code","execution_count":53,"metadata":{"execution":{"iopub.execute_input":"2024-02-16T07:52:15.626155Z","iopub.status.busy":"2024-02-16T07:52:15.624981Z","iopub.status.idle":"2024-02-16T07:52:15.636447Z","shell.execute_reply":"2024-02-16T07:52:15.635592Z","shell.execute_reply.started":"2024-02-16T07:52:15.626113Z"},"trusted":true},"outputs":[],"source":["# Nwow function \n","def chat_conversion(text):\n","    new_txt = []\n","    for w in text.split():\n","        if w.upper() in abbreviations_dict:\n","            new_txt.append(abbreviations_dict[w.upper()])\n","        else:\n","            new_txt.append(w)\n","    return \" \".join(new_txt)"]},{"cell_type":"code","execution_count":54,"metadata":{"execution":{"iopub.execute_input":"2024-02-16T07:52:16.061598Z","iopub.status.busy":"2024-02-16T07:52:16.060806Z","iopub.status.idle":"2024-02-16T07:52:16.069678Z","shell.execute_reply":"2024-02-16T07:52:16.068235Z","shell.execute_reply.started":"2024-02-16T07:52:16.061561Z"},"trusted":true},"outputs":[{"data":{"text/plain":["'In My Honest/Humble Opinion he is the best'"]},"execution_count":54,"metadata":{},"output_type":"execute_result"}],"source":["chat_conversion(\"IMHO he is the best\")"]},{"cell_type":"code","execution_count":55,"metadata":{"execution":{"iopub.execute_input":"2024-02-16T07:52:16.509950Z","iopub.status.busy":"2024-02-16T07:52:16.509439Z","iopub.status.idle":"2024-02-16T07:52:22.131166Z","shell.execute_reply":"2024-02-16T07:52:22.129965Z","shell.execute_reply.started":"2024-02-16T07:52:16.509889Z"},"trusted":true},"outputs":[],"source":["df['review'] = df['review'].apply(chat_conversion)"]},{"cell_type":"markdown","metadata":{},"source":["## Step 6\n","- Spelling correction "]},{"cell_type":"code","execution_count":56,"metadata":{"execution":{"iopub.execute_input":"2024-02-16T07:52:22.134408Z","iopub.status.busy":"2024-02-16T07:52:22.133799Z","iopub.status.idle":"2024-02-16T07:52:22.141650Z","shell.execute_reply":"2024-02-16T07:52:22.140002Z","shell.execute_reply.started":"2024-02-16T07:52:22.134375Z"},"trusted":true},"outputs":[],"source":["from textblob import TextBlob"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# import tensorflow as tf\n","# from textblob import TextBlob\n","\n","# # correct spellings using TextBlob\n","# def correct_spellings_textblob(text_tensor):\n","#     text = text_tensor.numpy().decode('utf-8')\n","#     corrected_text = str(TextBlob(text).correct())\n","#     return corrected_text\n","\n","# def correct_spellings_tf(text_tensor):\n","#     return tf.py_function(correct_spellings_textblob, [text_tensor], tf.string)\n","\n","# dataset = tf.data.Dataset.from_tensor_slices(data['review'])\n","# num_workers = 8\n","# corrected_reviews = dataset.map(correct_spellings_tf, num_parallel_calls=num_workers)\n","# corrected_reviews_values = list(corrected_reviews.as_numpy_iterator())\n","# data['corrected_review'] = corrected_reviews_values"]},{"cell_type":"code","execution_count":57,"metadata":{"execution":{"iopub.execute_input":"2024-02-16T07:52:22.143501Z","iopub.status.busy":"2024-02-16T07:52:22.143167Z","iopub.status.idle":"2024-02-16T08:04:56.956516Z","shell.execute_reply":"2024-02-16T08:04:56.954082Z","shell.execute_reply.started":"2024-02-16T07:52:22.143473Z"},"trusted":true},"outputs":[{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[57], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcorrected_review\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mreview\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mTextBlob\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcorrect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/core/series.py:4904\u001b[0m, in \u001b[0;36mSeries.apply\u001b[0;34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[0m\n\u001b[1;32m   4769\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[1;32m   4770\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   4771\u001b[0m     func: AggFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4776\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   4777\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[1;32m   4778\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   4779\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[1;32m   4780\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4895\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[1;32m   4896\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m   4897\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4898\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4899\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4900\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4901\u001b[0m \u001b[43m        \u001b[49m\u001b[43mby_row\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mby_row\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4902\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4903\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m-> 4904\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/core/apply.py:1427\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1424\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_compat()\n\u001b[1;32m   1426\u001b[0m \u001b[38;5;66;03m# self.func is Callable\u001b[39;00m\n\u001b[0;32m-> 1427\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/core/apply.py:1507\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1501\u001b[0m \u001b[38;5;66;03m# row-wise access\u001b[39;00m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m \u001b[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001b[39;00m\n\u001b[1;32m   1504\u001b[0m \u001b[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001b[39;00m\n\u001b[1;32m   1505\u001b[0m \u001b[38;5;66;03m#  Categorical (GH51645).\u001b[39;00m\n\u001b[1;32m   1506\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj\u001b[38;5;241m.\u001b[39mdtype, CategoricalDtype) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1507\u001b[0m mapped \u001b[38;5;241m=\u001b[39m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_map_values\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1508\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmapper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcurried\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\n\u001b[1;32m   1509\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[1;32m   1512\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[1;32m   1513\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[1;32m   1514\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/core/base.py:921\u001b[0m, in \u001b[0;36mIndexOpsMixin._map_values\u001b[0;34m(self, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m    918\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[1;32m    919\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mmap(mapper, na_action\u001b[38;5;241m=\u001b[39mna_action)\n\u001b[0;32m--> 921\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43malgorithms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mna_action\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/core/algorithms.py:1743\u001b[0m, in \u001b[0;36mmap_array\u001b[0;34m(arr, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m   1741\u001b[0m values \u001b[38;5;241m=\u001b[39m arr\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1743\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1745\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mmap_infer_mask(\n\u001b[1;32m   1746\u001b[0m         values, mapper, mask\u001b[38;5;241m=\u001b[39misna(values)\u001b[38;5;241m.\u001b[39mview(np\u001b[38;5;241m.\u001b[39muint8), convert\u001b[38;5;241m=\u001b[39mconvert\n\u001b[1;32m   1747\u001b[0m     )\n","File \u001b[0;32mlib.pyx:2972\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n","Cell \u001b[0;32mIn[57], line 1\u001b[0m, in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcorrected_review\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreview\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;28mstr\u001b[39m(\u001b[43mTextBlob\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcorrect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m))\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/textblob/blob.py:609\u001b[0m, in \u001b[0;36mBaseBlob.correct\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    607\u001b[0m tokens \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mtokenize\u001b[38;5;241m.\u001b[39mregexp_tokenize(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw, \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+|[^\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms]|\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    608\u001b[0m corrected \u001b[38;5;241m=\u001b[39m (Word(w)\u001b[38;5;241m.\u001b[39mcorrect() \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m tokens)\n\u001b[0;32m--> 609\u001b[0m ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcorrected\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    610\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m(ret)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/textblob/blob.py:608\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    606\u001b[0m \u001b[38;5;66;03m# regex matches: word or punctuation or whitespace\u001b[39;00m\n\u001b[1;32m    607\u001b[0m tokens \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mtokenize\u001b[38;5;241m.\u001b[39mregexp_tokenize(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw, \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+|[^\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms]|\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 608\u001b[0m corrected \u001b[38;5;241m=\u001b[39m (\u001b[43mWord\u001b[49m\u001b[43m(\u001b[49m\u001b[43mw\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcorrect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m tokens)\n\u001b[1;32m    609\u001b[0m ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(corrected)\n\u001b[1;32m    610\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m(ret)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/textblob/blob.py:142\u001b[0m, in \u001b[0;36mWord.correct\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcorrect\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    137\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m'''Correct the spelling of the word. Returns the word with the highest\u001b[39;00m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;124;03m    confidence using the spelling corrector.\u001b[39;00m\n\u001b[1;32m    139\u001b[0m \n\u001b[1;32m    140\u001b[0m \u001b[38;5;124;03m    .. versionadded:: 0.6.0\u001b[39;00m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;124;03m    '''\u001b[39;00m\n\u001b[0;32m--> 142\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Word(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mspellcheck\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m])\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/textblob/blob.py:134\u001b[0m, in \u001b[0;36mWord.spellcheck\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mspellcheck\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    126\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m'''Return a list of (word, confidence) tuples of spelling corrections.\u001b[39;00m\n\u001b[1;32m    127\u001b[0m \n\u001b[1;32m    128\u001b[0m \u001b[38;5;124;03m    Based on: Peter Norvig, \"How to Write a Spelling Corrector\"\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;124;03m    .. versionadded:: 0.6.0\u001b[39;00m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;124;03m    '''\u001b[39;00m\n\u001b[0;32m--> 134\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msuggest\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstring\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/textblob/en/__init__.py:123\u001b[0m, in \u001b[0;36msuggest\u001b[0;34m(w)\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msuggest\u001b[39m(w):\n\u001b[1;32m    121\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\" Returns a list of (word, confidence)-tuples of spelling corrections.\u001b[39;00m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 123\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mspelling\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msuggest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mw\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/textblob/_text.py:1399\u001b[0m, in \u001b[0;36mSpelling.suggest\u001b[0;34m(self, w)\u001b[0m\n\u001b[1;32m   1395\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m w\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39misdigit():\n\u001b[1;32m   1396\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [(w, \u001b[38;5;241m1.0\u001b[39m)] \u001b[38;5;66;03m# 1.5\u001b[39;00m\n\u001b[1;32m   1397\u001b[0m candidates \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_known([w]) \\\n\u001b[1;32m   1398\u001b[0m           \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_known(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_edit1(w)) \\\n\u001b[0;32m-> 1399\u001b[0m           \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_known(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_edit2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mw\u001b[49m\u001b[43m)\u001b[49m) \\\n\u001b[1;32m   1400\u001b[0m           \u001b[38;5;129;01mor\u001b[39;00m [w]\n\u001b[1;32m   1401\u001b[0m candidates \u001b[38;5;241m=\u001b[39m [(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget(c, \u001b[38;5;241m0.0\u001b[39m), c) \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m candidates]\n\u001b[1;32m   1402\u001b[0m s \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;28msum\u001b[39m(p \u001b[38;5;28;01mfor\u001b[39;00m p, word \u001b[38;5;129;01min\u001b[39;00m candidates) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;241m1\u001b[39m)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/textblob/_text.py:1376\u001b[0m, in \u001b[0;36mSpelling._edit2\u001b[0;34m(self, w)\u001b[0m\n\u001b[1;32m   1372\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\" Returns a set of words with edit distance 2 from the given word\u001b[39;00m\n\u001b[1;32m   1373\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1374\u001b[0m \u001b[38;5;66;03m# Of all spelling errors, 99% is covered by edit distance 2.\u001b[39;00m\n\u001b[1;32m   1375\u001b[0m \u001b[38;5;66;03m# Only keep candidates that are actually known words (20% speedup).\u001b[39;00m\n\u001b[0;32m-> 1376\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mset\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43me2\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43me1\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_edit1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mw\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43me2\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_edit1\u001b[49m\u001b[43m(\u001b[49m\u001b[43me1\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43me2\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/textblob/_text.py:1376\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1372\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\" Returns a set of words with edit distance 2 from the given word\u001b[39;00m\n\u001b[1;32m   1373\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1374\u001b[0m \u001b[38;5;66;03m# Of all spelling errors, 99% is covered by edit distance 2.\u001b[39;00m\n\u001b[1;32m   1375\u001b[0m \u001b[38;5;66;03m# Only keep candidates that are actually known words (20% speedup).\u001b[39;00m\n\u001b[0;32m-> 1376\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mset\u001b[39m(e2 \u001b[38;5;28;01mfor\u001b[39;00m e1 \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_edit1(w) \u001b[38;5;28;01mfor\u001b[39;00m e2 \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_edit1(e1) \u001b[38;5;28;01mif\u001b[39;00m e2 \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m)\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["df['corrected_review'] = df['review'].apply(lambda x: str(TextBlob(str(x)).correct()))\n"]},{"cell_type":"markdown","metadata":{},"source":["## Removing stop words\n","- words like a, the, of, are,any etc. are not usefull in santiment analysis but in  parts of tagging, these kind of words are usefull."]},{"cell_type":"code","execution_count":58,"metadata":{"execution":{"iopub.execute_input":"2024-02-16T08:05:01.487545Z","iopub.status.busy":"2024-02-16T08:05:01.487141Z","iopub.status.idle":"2024-02-16T08:05:01.493087Z","shell.execute_reply":"2024-02-16T08:05:01.491471Z","shell.execute_reply.started":"2024-02-16T08:05:01.487515Z"},"trusted":true},"outputs":[],"source":["from nltk.corpus import stopwords"]},{"cell_type":"code","execution_count":59,"metadata":{"execution":{"iopub.execute_input":"2024-02-16T08:05:55.921686Z","iopub.status.busy":"2024-02-16T08:05:55.921271Z","iopub.status.idle":"2024-02-16T08:05:55.941254Z","shell.execute_reply":"2024-02-16T08:05:55.940095Z","shell.execute_reply.started":"2024-02-16T08:05:55.921657Z"},"trusted":true},"outputs":[{"data":{"text/plain":["['i',\n"," 'me',\n"," 'my',\n"," 'myself',\n"," 'we',\n"," 'our',\n"," 'ours',\n"," 'ourselves',\n"," 'you',\n"," \"you're\",\n"," \"you've\",\n"," \"you'll\",\n"," \"you'd\",\n"," 'your',\n"," 'yours',\n"," 'yourself',\n"," 'yourselves',\n"," 'he',\n"," 'him',\n"," 'his',\n"," 'himself',\n"," 'she',\n"," \"she's\",\n"," 'her',\n"," 'hers',\n"," 'herself',\n"," 'it',\n"," \"it's\",\n"," 'its',\n"," 'itself',\n"," 'they',\n"," 'them',\n"," 'their',\n"," 'theirs',\n"," 'themselves',\n"," 'what',\n"," 'which',\n"," 'who',\n"," 'whom',\n"," 'this',\n"," 'that',\n"," \"that'll\",\n"," 'these',\n"," 'those',\n"," 'am',\n"," 'is',\n"," 'are',\n"," 'was',\n"," 'were',\n"," 'be',\n"," 'been',\n"," 'being',\n"," 'have',\n"," 'has',\n"," 'had',\n"," 'having',\n"," 'do',\n"," 'does',\n"," 'did',\n"," 'doing',\n"," 'a',\n"," 'an',\n"," 'the',\n"," 'and',\n"," 'but',\n"," 'if',\n"," 'or',\n"," 'because',\n"," 'as',\n"," 'until',\n"," 'while',\n"," 'of',\n"," 'at',\n"," 'by',\n"," 'for',\n"," 'with',\n"," 'about',\n"," 'against',\n"," 'between',\n"," 'into',\n"," 'through',\n"," 'during',\n"," 'before',\n"," 'after',\n"," 'above',\n"," 'below',\n"," 'to',\n"," 'from',\n"," 'up',\n"," 'down',\n"," 'in',\n"," 'out',\n"," 'on',\n"," 'off',\n"," 'over',\n"," 'under',\n"," 'again',\n"," 'further',\n"," 'then',\n"," 'once',\n"," 'here',\n"," 'there',\n"," 'when',\n"," 'where',\n"," 'why',\n"," 'how',\n"," 'all',\n"," 'any',\n"," 'both',\n"," 'each',\n"," 'few',\n"," 'more',\n"," 'most',\n"," 'other',\n"," 'some',\n"," 'such',\n"," 'no',\n"," 'nor',\n"," 'not',\n"," 'only',\n"," 'own',\n"," 'same',\n"," 'so',\n"," 'than',\n"," 'too',\n"," 'very',\n"," 's',\n"," 't',\n"," 'can',\n"," 'will',\n"," 'just',\n"," 'don',\n"," \"don't\",\n"," 'should',\n"," \"should've\",\n"," 'now',\n"," 'd',\n"," 'll',\n"," 'm',\n"," 'o',\n"," 're',\n"," 've',\n"," 'y',\n"," 'ain',\n"," 'aren',\n"," \"aren't\",\n"," 'couldn',\n"," \"couldn't\",\n"," 'didn',\n"," \"didn't\",\n"," 'doesn',\n"," \"doesn't\",\n"," 'hadn',\n"," \"hadn't\",\n"," 'hasn',\n"," \"hasn't\",\n"," 'haven',\n"," \"haven't\",\n"," 'isn',\n"," \"isn't\",\n"," 'ma',\n"," 'mightn',\n"," \"mightn't\",\n"," 'mustn',\n"," \"mustn't\",\n"," 'needn',\n"," \"needn't\",\n"," 'shan',\n"," \"shan't\",\n"," 'shouldn',\n"," \"shouldn't\",\n"," 'wasn',\n"," \"wasn't\",\n"," 'weren',\n"," \"weren't\",\n"," 'won',\n"," \"won't\",\n"," 'wouldn',\n"," \"wouldn't\"]"]},"execution_count":59,"metadata":{},"output_type":"execute_result"}],"source":["stopwords.words('english')"]},{"cell_type":"code","execution_count":60,"metadata":{"execution":{"iopub.execute_input":"2024-02-16T08:46:46.218707Z","iopub.status.busy":"2024-02-16T08:46:46.218214Z","iopub.status.idle":"2024-02-16T08:46:46.227110Z","shell.execute_reply":"2024-02-16T08:46:46.225712Z","shell.execute_reply.started":"2024-02-16T08:46:46.218672Z"},"trusted":true},"outputs":[],"source":["def remove_stopwords(text):\n","    new_txt = []\n","    \n","    for word in text.split():\n","        if word in stopwords.words('english'):\n","            new_txt.append('')\n","        else:\n","            new_txt.append(word)\n","    x = new_txt[:]\n","    new_txt.clear()\n","    return \" \".join(x)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-02-16T09:06:08.789129Z","iopub.status.idle":"2024-02-16T09:06:08.789802Z","shell.execute_reply":"2024-02-16T09:06:08.789507Z","shell.execute_reply.started":"2024-02-16T09:06:08.789480Z"},"trusted":true},"outputs":[],"source":["df['review'].apply(remove_stopwords)"]},{"cell_type":"markdown","metadata":{},"source":["## Handling emojis\n","- option1 - replace it with meaning\n","- option2 - replace with text"]},{"cell_type":"code","execution_count":63,"metadata":{"execution":{"iopub.execute_input":"2024-02-16T09:06:12.981652Z","iopub.status.busy":"2024-02-16T09:06:12.980961Z","iopub.status.idle":"2024-02-16T09:06:12.989844Z","shell.execute_reply":"2024-02-16T09:06:12.988322Z","shell.execute_reply.started":"2024-02-16T09:06:12.981620Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["This dog 😂\n","This dog \n"]}],"source":["# Fro removing emojis we can use \n","#!/usr/bin/env python\n","import re\n","\n","text = u'This dog \\U0001f602'\n","print(text)\n","\n","emoji_pattern = re.compile(\"[\"\n","        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n","        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n","        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n","        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n","                           \"]+\", flags=re.UNICODE)\n","print(emoji_pattern.sub(r'', text)) # no emoji"]},{"cell_type":"code","execution_count":66,"metadata":{"execution":{"iopub.execute_input":"2024-02-16T09:07:49.520036Z","iopub.status.busy":"2024-02-16T09:07:49.519577Z","iopub.status.idle":"2024-02-16T09:07:49.526843Z","shell.execute_reply":"2024-02-16T09:07:49.525201Z","shell.execute_reply.started":"2024-02-16T09:07:49.520004Z"},"trusted":true},"outputs":[],"source":["def remove_emoji(text):\n","    emoji_pattern = re.compile(\"[\"\n","        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n","        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n","        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n","        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n","                           \"]+\", flags=re.UNICODE)\n","    return emoji_pattern.sub(r'',text)\n"]},{"cell_type":"code","execution_count":67,"metadata":{"execution":{"iopub.execute_input":"2024-02-16T09:07:50.463265Z","iopub.status.busy":"2024-02-16T09:07:50.462858Z","iopub.status.idle":"2024-02-16T09:07:54.640523Z","shell.execute_reply":"2024-02-16T09:07:54.639391Z","shell.execute_reply.started":"2024-02-16T09:07:50.463236Z"},"trusted":true},"outputs":[{"data":{"text/plain":["0        one of the other reviewers has mentioned that ...\n","1        a wonderful little production the filming tech...\n","2        i thought this was a wonderful way to spend Te...\n","3        basically theres a family where a little boy j...\n","4        petter matteis love in the Tears in my eyes of...\n","                               ...                        \n","49995    i thought this movie did a down right good job...\n","49996    bad plot bad dialogue bad acting idiotic direc...\n","49997    i am a catholic taught in parochial elementary...\n","49998    im going to have to disagree with the previous...\n","49999    no one expects the star trek movies to be high...\n","Name: review, Length: 50000, dtype: object"]},"execution_count":67,"metadata":{},"output_type":"execute_result"}],"source":["df['review'].apply(remove_emoji)"]},{"cell_type":"markdown","metadata":{},"source":["replace with meaning"]},{"cell_type":"code","execution_count":68,"metadata":{"execution":{"iopub.execute_input":"2024-02-16T09:10:41.004471Z","iopub.status.busy":"2024-02-16T09:10:41.003887Z","iopub.status.idle":"2024-02-16T09:10:41.085620Z","shell.execute_reply":"2024-02-16T09:10:41.084654Z","shell.execute_reply.started":"2024-02-16T09:10:41.004435Z"},"trusted":true},"outputs":[],"source":["import emoji\n","def de_emojise(text):\n","    return emoji.demojize(text)"]},{"cell_type":"code","execution_count":69,"metadata":{"execution":{"iopub.execute_input":"2024-02-16T09:11:35.579815Z","iopub.status.busy":"2024-02-16T09:11:35.578880Z","iopub.status.idle":"2024-02-16T09:13:44.436289Z","shell.execute_reply":"2024-02-16T09:13:44.435240Z","shell.execute_reply.started":"2024-02-16T09:11:35.579779Z"},"trusted":true},"outputs":[],"source":["df['review'] = df['review'].apply(de_emojise)"]},{"cell_type":"markdown","metadata":{},"source":["## Tokenization\n","- Prefix : character(s) at the beginning $,(,',\"\n","- Suffix : Character(s) at the endkm),.!\"\n","- Infix : Character(s) in between _ __/ ...\n","- Exception : Spacial- case rule to split a string into several tokens from being split when punctuation relues are applied (let U.S.)"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2024-02-17T06:23:58.991374Z","iopub.status.busy":"2024-02-17T06:23:58.991000Z","iopub.status.idle":"2024-02-17T06:23:58.995155Z","shell.execute_reply":"2024-02-17T06:23:58.994326Z","shell.execute_reply.started":"2024-02-17T06:23:58.991345Z"},"trusted":true},"outputs":[],"source":["import nltk\n","from nltk.tokenize import word_tokenize, sent_tokenize"]},{"cell_type":"code","execution_count":80,"metadata":{"execution":{"iopub.execute_input":"2024-02-16T10:00:11.590379Z","iopub.status.busy":"2024-02-16T10:00:11.589103Z","iopub.status.idle":"2024-02-16T10:02:58.187658Z","shell.execute_reply":"2024-02-16T10:02:58.186219Z","shell.execute_reply.started":"2024-02-16T10:00:11.590327Z"},"trusted":true},"outputs":[],"source":["#nltk.download('punkt')\n","df['sentences'] = df['review'].apply(word_tokenize)"]},{"cell_type":"code","execution_count":81,"metadata":{"execution":{"iopub.execute_input":"2024-02-16T10:02:58.190828Z","iopub.status.busy":"2024-02-16T10:02:58.190375Z","iopub.status.idle":"2024-02-16T10:02:58.208276Z","shell.execute_reply":"2024-02-16T10:02:58.206811Z","shell.execute_reply.started":"2024-02-16T10:02:58.190788Z"},"trusted":true},"outputs":[{"data":{"text/plain":["['One',\n"," 'of',\n"," 'the',\n"," 'other',\n"," 'reviewers',\n"," 'has',\n"," 'mentioned',\n"," 'that',\n"," 'after',\n"," 'watching',\n"," 'just',\n"," '1',\n"," 'Oz',\n"," 'episode',\n"," 'you',\n"," \"'ll\",\n"," 'be',\n"," 'hooked',\n"," '.',\n"," 'They',\n"," 'are',\n"," 'right',\n"," ',',\n"," 'as',\n"," 'this',\n"," 'is',\n"," 'exactly',\n"," 'what',\n"," 'happened',\n"," 'with',\n"," 'me.',\n"," '<',\n"," 'br',\n"," '/',\n"," '>',\n"," '<',\n"," 'br',\n"," '/',\n"," '>',\n"," 'The',\n"," 'first',\n"," 'thing',\n"," 'that',\n"," 'struck',\n"," 'me',\n"," 'about',\n"," 'Oz',\n"," 'was',\n"," 'its',\n"," 'brutality',\n"," 'and',\n"," 'unflinching',\n"," 'scenes',\n"," 'of',\n"," 'violence',\n"," ',',\n"," 'which',\n"," 'set',\n"," 'in',\n"," 'right',\n"," 'from',\n"," 'the',\n"," 'word',\n"," 'GO',\n"," '.',\n"," 'Trust',\n"," 'me',\n"," ',',\n"," 'this',\n"," 'is',\n"," 'not',\n"," 'a',\n"," 'show',\n"," 'for',\n"," 'the',\n"," 'faint',\n"," 'hearted',\n"," 'or',\n"," 'timid',\n"," '.',\n"," 'This',\n"," 'show',\n"," 'pulls',\n"," 'no',\n"," 'punches',\n"," 'with',\n"," 'regards',\n"," 'to',\n"," 'drugs',\n"," ',',\n"," 'sex',\n"," 'or',\n"," 'violence',\n"," '.',\n"," 'Its',\n"," 'is',\n"," 'hardcore',\n"," ',',\n"," 'in',\n"," 'the',\n"," 'classic',\n"," 'use',\n"," 'of',\n"," 'the',\n"," 'word.',\n"," '<',\n"," 'br',\n"," '/',\n"," '>',\n"," '<',\n"," 'br',\n"," '/',\n"," '>',\n"," 'It',\n"," 'is',\n"," 'called',\n"," 'OZ',\n"," 'as',\n"," 'that',\n"," 'is',\n"," 'the',\n"," 'nickname',\n"," 'given',\n"," 'to',\n"," 'the',\n"," 'Oswald',\n"," 'Maximum',\n"," 'Security',\n"," 'State',\n"," 'Penitentary',\n"," '.',\n"," 'It',\n"," 'focuses',\n"," 'mainly',\n"," 'on',\n"," 'Emerald',\n"," 'City',\n"," ',',\n"," 'an',\n"," 'experimental',\n"," 'section',\n"," 'of',\n"," 'the',\n"," 'prison',\n"," 'where',\n"," 'all',\n"," 'the',\n"," 'cells',\n"," 'have',\n"," 'glass',\n"," 'fronts',\n"," 'and',\n"," 'face',\n"," 'inwards',\n"," ',',\n"," 'so',\n"," 'privacy',\n"," 'is',\n"," 'not',\n"," 'high',\n"," 'on',\n"," 'the',\n"," 'agenda',\n"," '.',\n"," 'Em',\n"," 'City',\n"," 'is',\n"," 'home',\n"," 'to',\n"," 'many..Aryans',\n"," ',',\n"," 'Muslims',\n"," ',',\n"," 'gangstas',\n"," ',',\n"," 'Latinos',\n"," ',',\n"," 'Christians',\n"," ',',\n"," 'Italians',\n"," ',',\n"," 'Irish',\n"," 'and',\n"," 'more',\n"," '...',\n"," '.so',\n"," 'scuffles',\n"," ',',\n"," 'death',\n"," 'stares',\n"," ',',\n"," 'dodgy',\n"," 'dealings',\n"," 'and',\n"," 'shady',\n"," 'agreements',\n"," 'are',\n"," 'never',\n"," 'far',\n"," 'away.',\n"," '<',\n"," 'br',\n"," '/',\n"," '>',\n"," '<',\n"," 'br',\n"," '/',\n"," '>',\n"," 'I',\n"," 'would',\n"," 'say',\n"," 'the',\n"," 'main',\n"," 'appeal',\n"," 'of',\n"," 'the',\n"," 'show',\n"," 'is',\n"," 'due',\n"," 'to',\n"," 'the',\n"," 'fact',\n"," 'that',\n"," 'it',\n"," 'goes',\n"," 'where',\n"," 'other',\n"," 'shows',\n"," 'would',\n"," \"n't\",\n"," 'dare',\n"," '.',\n"," 'Forget',\n"," 'pretty',\n"," 'pictures',\n"," 'painted',\n"," 'for',\n"," 'mainstream',\n"," 'audiences',\n"," ',',\n"," 'forget',\n"," 'charm',\n"," ',',\n"," 'forget',\n"," 'romance',\n"," '...',\n"," 'OZ',\n"," 'does',\n"," \"n't\",\n"," 'mess',\n"," 'around',\n"," '.',\n"," 'The',\n"," 'first',\n"," 'episode',\n"," 'I',\n"," 'ever',\n"," 'saw',\n"," 'struck',\n"," 'me',\n"," 'as',\n"," 'so',\n"," 'nasty',\n"," 'it',\n"," 'was',\n"," 'surreal',\n"," ',',\n"," 'I',\n"," 'could',\n"," \"n't\",\n"," 'say',\n"," 'I',\n"," 'was',\n"," 'ready',\n"," 'for',\n"," 'it',\n"," ',',\n"," 'but',\n"," 'as',\n"," 'I',\n"," 'watched',\n"," 'more',\n"," ',',\n"," 'I',\n"," 'developed',\n"," 'a',\n"," 'taste',\n"," 'for',\n"," 'Oz',\n"," ',',\n"," 'and',\n"," 'got',\n"," 'accustomed',\n"," 'to',\n"," 'the',\n"," 'high',\n"," 'levels',\n"," 'of',\n"," 'graphic',\n"," 'violence',\n"," '.',\n"," 'Not',\n"," 'just',\n"," 'violence',\n"," ',',\n"," 'but',\n"," 'injustice',\n"," '(',\n"," 'crooked',\n"," 'guards',\n"," 'who',\n"," \"'ll\",\n"," 'be',\n"," 'sold',\n"," 'out',\n"," 'for',\n"," 'a',\n"," 'nickel',\n"," ',',\n"," 'inmates',\n"," 'who',\n"," \"'ll\",\n"," 'kill',\n"," 'on',\n"," 'order',\n"," 'and',\n"," 'get',\n"," 'away',\n"," 'with',\n"," 'it',\n"," ',',\n"," 'well',\n"," 'mannered',\n"," ',',\n"," 'middle',\n"," 'class',\n"," 'inmates',\n"," 'being',\n"," 'turned',\n"," 'into',\n"," 'prison',\n"," 'bitches',\n"," 'due',\n"," 'to',\n"," 'their',\n"," 'lack',\n"," 'of',\n"," 'street',\n"," 'skills',\n"," 'or',\n"," 'prison',\n"," 'experience',\n"," ')',\n"," 'Watching',\n"," 'Oz',\n"," ',',\n"," 'you',\n"," 'may',\n"," 'become',\n"," 'comfortable',\n"," 'with',\n"," 'what',\n"," 'is',\n"," 'uncomfortable',\n"," 'viewing',\n"," '...',\n"," '.thats',\n"," 'if',\n"," 'you',\n"," 'can',\n"," 'get',\n"," 'in',\n"," 'touch',\n"," 'with',\n"," 'your',\n"," 'darker',\n"," 'side',\n"," '.']"]},"execution_count":81,"metadata":{},"output_type":"execute_result"}],"source":["df['sentences'][0]"]},{"cell_type":"code","execution_count":83,"metadata":{"execution":{"iopub.execute_input":"2024-02-16T10:05:37.188407Z","iopub.status.busy":"2024-02-16T10:05:37.187935Z","iopub.status.idle":"2024-02-16T10:05:38.450773Z","shell.execute_reply":"2024-02-16T10:05:38.449415Z","shell.execute_reply.started":"2024-02-16T10:05:37.188375Z"},"trusted":true},"outputs":[],"source":["import spacy\n","nlp = spacy.load('en_core_web_sm')\n","def word_tokenization(text):\n","    doc = nlp(text)\n","    return doc\n","    "]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-02-16T10:34:26.523165Z","iopub.status.idle":"2024-02-16T10:34:26.524481Z","shell.execute_reply":"2024-02-16T10:34:26.524215Z","shell.execute_reply.started":"2024-02-16T10:34:26.524191Z"},"trusted":true},"outputs":[],"source":["df['words'] = df['review'].apply(word_tokenization)"]},{"cell_type":"code","execution_count":85,"metadata":{"execution":{"iopub.execute_input":"2024-02-16T10:34:28.511405Z","iopub.status.busy":"2024-02-16T10:34:28.510984Z","iopub.status.idle":"2024-02-16T10:34:28.661691Z","shell.execute_reply":"2024-02-16T10:34:28.659976Z","shell.execute_reply.started":"2024-02-16T10:34:28.511375Z"},"trusted":true},"outputs":[{"ename":"KeyError","evalue":"'words'","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/core/indexes/base.py:3802\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3801\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3802\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3803\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n","File \u001b[0;32mindex.pyx:153\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n","File \u001b[0;32mindex.pyx:182\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n","File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n","File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n","\u001b[0;31mKeyError\u001b[0m: 'words'","\nThe above exception was the direct cause of the following exception:\n","\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)","Cell \u001b[0;32mIn[85], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mwords\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/core/frame.py:4090\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4088\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   4089\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 4090\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4091\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   4092\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/core/indexes/base.py:3809\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3804\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m   3805\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m   3806\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[1;32m   3807\u001b[0m     ):\n\u001b[1;32m   3808\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[0;32m-> 3809\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3810\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3811\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3812\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3813\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3814\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n","\u001b[0;31mKeyError\u001b[0m: 'words'"]}],"source":["df['words'][0]"]},{"cell_type":"markdown","metadata":{},"source":["## Stemming \n","* In grammer, inflection is the modification of a word to exprees diffrent grammatical categoris such as tense, case, voice, aspect, person, number, gender and mood.\n","* Stemming is a process of reducing inflection in words to their root forms such as mapping a group of words to the same stem even if the stem itself is not a valid word in the Language\n","- stemmer : (1). Porter stemmer(specially for englist language) (2). Snowball stemmer(for other language)"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-02-16T11:09:25.518630Z","iopub.status.busy":"2024-02-16T11:09:25.517747Z","iopub.status.idle":"2024-02-16T11:09:26.813178Z","shell.execute_reply":"2024-02-16T11:09:26.811965Z","shell.execute_reply.started":"2024-02-16T11:09:25.518598Z"},"trusted":true},"outputs":[],"source":["from nltk.stem.porter import PorterStemmer"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-02-16T11:09:26.819372Z","iopub.status.busy":"2024-02-16T11:09:26.818689Z","iopub.status.idle":"2024-02-16T11:09:26.826065Z","shell.execute_reply":"2024-02-16T11:09:26.823832Z","shell.execute_reply.started":"2024-02-16T11:09:26.819340Z"},"trusted":true},"outputs":[],"source":["ps = PorterStemmer()\n","def stem_words(text):\n","    return \" \".join([ps.stem(word) for word in text.split()])"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-02-16T11:09:27.903864Z","iopub.status.busy":"2024-02-16T11:09:27.903146Z","iopub.status.idle":"2024-02-16T11:09:27.913556Z","shell.execute_reply":"2024-02-16T11:09:27.912671Z","shell.execute_reply.started":"2024-02-16T11:09:27.903829Z"},"trusted":true},"outputs":[{"data":{"text/plain":["\"One of the other reviewers has mentioned that after watching just 1 Oz episode you'll be hooked. They are right, as this is exactly what happened with me.<br /><br />The first thing that struck me about Oz was its brutality and unflinching scenes of violence, which set in right from the word GO. Trust me, this is not a show for the faint hearted or timid. This show pulls no punches with regards to drugs, sex or violence. Its is hardcore, in the classic use of the word.<br /><br />It is called OZ as that is the nickname given to the Oswald Maximum Security State Penitentary. It focuses mainly on Emerald City, an experimental section of the prison where all the cells have glass fronts and face inwards, so privacy is not high on the agenda. Em City is home to many..Aryans, Muslims, gangstas, Latinos, Christians, Italians, Irish and more....so scuffles, death stares, dodgy dealings and shady agreements are never far away.<br /><br />I would say the main appeal of the show is due to the fact that it goes where other shows wouldn't dare. Forget pretty pictures painted for mainstream audiences, forget charm, forget romance...OZ doesn't mess around. The first episode I ever saw struck me as so nasty it was surreal, I couldn't say I was ready for it, but as I watched more, I developed a taste for Oz, and got accustomed to the high levels of graphic violence. Not just violence, but injustice (crooked guards who'll be sold out for a nickel, inmates who'll kill on order and get away with it, well mannered, middle class inmates being turned into prison bitches due to their lack of street skills or prison experience) Watching Oz, you may become comfortable with what is uncomfortable viewing....thats if you can get in touch with your darker side.\""]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["df['review'][0]"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-02-16T11:09:29.503610Z","iopub.status.busy":"2024-02-16T11:09:29.502799Z","iopub.status.idle":"2024-02-16T11:09:29.545279Z","shell.execute_reply":"2024-02-16T11:09:29.544085Z","shell.execute_reply.started":"2024-02-16T11:09:29.503559Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["one of the other review ha mention that after watch just 1 Oz episod you'll be hooked. they are right, as thi is exactli what happen with me.<br /><br />the first thing that struck me about Oz wa it brutal and unflinch scene of violence, which set in right from the word go. trust me, thi is not a show for the faint heart or timid. thi show pull no punch with regard to drugs, sex or violence. it is hardcore, in the classic use of the word.<br /><br />it is call OZ as that is the nicknam given to the oswald maximum secur state penitentary. It focus mainli on emerald city, an experiment section of the prison where all the cell have glass front and face inwards, so privaci is not high on the agenda. Em citi is home to many..aryans, muslims, gangstas, latinos, christians, italians, irish and more....so scuffles, death stares, dodgi deal and shadi agreement are never far away.<br /><br />i would say the main appeal of the show is due to the fact that it goe where other show wouldn't dare. forget pretti pictur paint for mainstream audiences, forget charm, forget romance...oz doesn't mess around. the first episod I ever saw struck me as so nasti it wa surreal, I couldn't say I wa readi for it, but as I watch more, I develop a tast for oz, and got accustom to the high level of graphic violence. not just violence, but injustic (crook guard who'll be sold out for a nickel, inmat who'll kill on order and get away with it, well mannered, middl class inmat be turn into prison bitch due to their lack of street skill or prison experience) watch oz, you may becom comfort with what is uncomfort viewing....that if you can get in touch with your darker side.\n"]}],"source":["print(df['review'][0:5].apply(stem_words)[ 0])"]},{"cell_type":"markdown","metadata":{},"source":["## Lemmatization :\n","Lemmatization,unlike Stemming, reduces the inflected words properly ensuring that the root word belong to the language. In Lemmatization root word is called Lemma . A lemma (plural lemmas or lemmata) is the canonical form, dictionary form, or citation form of a set of words."]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2024-02-17T06:20:31.606081Z","iopub.status.busy":"2024-02-17T06:20:31.605700Z","iopub.status.idle":"2024-02-17T06:20:33.989068Z","shell.execute_reply":"2024-02-17T06:20:33.988237Z","shell.execute_reply.started":"2024-02-17T06:20:31.606048Z"},"trusted":true},"outputs":[],"source":["from nltk.stem import WordNetLemmatizer\n","word_lemmatizer = WordNetLemmatizer()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def lammatization(text):\n","    words=text.split()\n","\n","    lemmetizer=WordNetLemmatizer()\n","\n","    lemetized_word=[lemmetizer.lemmatize(word) for word in words]\n","    \n","    return lemetized_word"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2024-02-17T06:22:15.615982Z","iopub.status.busy":"2024-02-17T06:22:15.615623Z","iopub.status.idle":"2024-02-17T06:22:15.620824Z","shell.execute_reply":"2024-02-17T06:22:15.619585Z","shell.execute_reply.started":"2024-02-17T06:22:15.615955Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["basically theres a family where a little boy jake thinks theres a zombie in his closet  his parents are fighting all the timethis movie is slower than a soap opera and suddenly jake decides to become rambo and kill the zombieok first of all when youre going to make a film you must decide if its a thriller or a drama as a drama the movie is watchable parents are divorcing  arguing like in real life and then we have jake with his closet which totally ruins all the film i expected to see a boogeyman similar movie and instead i watched a drama with some meaningless thriller spots3 out of 10 just for the well playing parents  descent dialogs as for the shots with jake just ignore them\n"]}],"source":["sentance = df['review'][3]\n","print(sentance)"]},{"cell_type":"code","execution_count":28,"metadata":{"execution":{"iopub.execute_input":"2024-02-17T06:38:08.617575Z","iopub.status.busy":"2024-02-17T06:38:08.617219Z","iopub.status.idle":"2024-02-17T06:38:08.623579Z","shell.execute_reply":"2024-02-17T06:38:08.622196Z","shell.execute_reply.started":"2024-02-17T06:38:08.617549Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["['/root/nltk_data', '/usr/share/nltk_data', '/usr/local/share/nltk_data', '/usr/lib/nltk_data', '/usr/local/lib/nltk_data', '/usr/local/share/nltk_data']\n"]}],"source":["import nltk\n","print(nltk.data.path)\n"]},{"cell_type":"code","execution_count":27,"metadata":{"execution":{"iopub.execute_input":"2024-02-17T06:37:20.872708Z","iopub.status.busy":"2024-02-17T06:37:20.872357Z","iopub.status.idle":"2024-02-17T06:37:20.972580Z","shell.execute_reply":"2024-02-17T06:37:20.971651Z","shell.execute_reply.started":"2024-02-17T06:37:20.872682Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Word                Lemma               \n"]},{"ename":"LookupError","evalue":"\n**********************************************************************\n  Resource 'corpora/wordnet' not found.  Please use the NLTK\n  Downloader to obtain the resource:  >>> nltk.download()\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - '/usr/local/share/nltk_data'\n**********************************************************************","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/nltk/corpus/util.py:80\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m---> 80\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m: root \u001b[38;5;241m=\u001b[39m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;132;43;01m{}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;132;43;01m{}\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubdir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mzip_name\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     81\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m: \u001b[38;5;28;01mraise\u001b[39;00m e\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/nltk/data.py:653\u001b[0m, in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    652\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m (sep, msg, sep)\n\u001b[0;32m--> 653\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n","\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource 'corpora/wordnet.zip/wordnet/.zip/' not found.  Please\n  use the NLTK Downloader to obtain the resource:  >>>\n  nltk.download()\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - '/usr/local/share/nltk_data'\n**********************************************************************","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)","Cell \u001b[0;32mIn[27], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{0:20}\u001b[39;00m\u001b[38;5;132;01m{1:20}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWord\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLemma\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m sentance_words:\n\u001b[0;32m----> 4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{0:20}\u001b[39;00m\u001b[38;5;132;01m{1:20}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(word,\u001b[43mword_lemmatizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlemmatize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mword\u001b[49m\u001b[43m,\u001b[49m\u001b[43mpos\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m))\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/nltk/stem/wordnet.py:40\u001b[0m, in \u001b[0;36mWordNetLemmatizer.lemmatize\u001b[0;34m(self, word, pos)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlemmatize\u001b[39m(\u001b[38;5;28mself\u001b[39m, word, pos\u001b[38;5;241m=\u001b[39mNOUN):\n\u001b[0;32m---> 40\u001b[0m     lemmas \u001b[38;5;241m=\u001b[39m \u001b[43mwordnet\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_morphy\u001b[49m(word, pos)\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mmin\u001b[39m(lemmas, key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m lemmas \u001b[38;5;28;01melse\u001b[39;00m word\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/nltk/corpus/util.py:116\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attr \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__bases__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLazyCorpusLoader object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__bases__\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 116\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__load\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;66;03m# This looks circular, but its not, since __load() changes our\u001b[39;00m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;66;03m# __class__ to something new:\u001b[39;00m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, attr)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/nltk/corpus/util.py:81\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     80\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m: root \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msubdir, zip_name))\n\u001b[0;32m---> 81\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m: \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m     83\u001b[0m \u001b[38;5;66;03m# Load the corpus.\u001b[39;00m\n\u001b[1;32m     84\u001b[0m corpus \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__reader_cls(root, \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__kwargs)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/nltk/corpus/util.py:78\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     77\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 78\u001b[0m         root \u001b[38;5;241m=\u001b[39m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;132;43;01m{}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;132;43;01m{}\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubdir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__name\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     79\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     80\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m: root \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msubdir, zip_name))\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/nltk/data.py:653\u001b[0m, in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    651\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[1;32m    652\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m (sep, msg, sep)\n\u001b[0;32m--> 653\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n","\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource 'corpora/wordnet' not found.  Please use the NLTK\n  Downloader to obtain the resource:  >>> nltk.download()\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - '/usr/local/share/nltk_data'\n**********************************************************************"]}],"source":["sentance_words = nltk.word_tokenize(sentance)\n","print('{0:20}{1:20}'.format('Word','Lemma'))\n","for word in sentance_words:\n","    print(\"{0:20}{1:20}\".format(word,word_lemmatizer.lemmatize(word,pos = 'v')))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"datasetId":134715,"sourceId":320111,"sourceType":"datasetVersion"}],"dockerImageVersionId":30646,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
